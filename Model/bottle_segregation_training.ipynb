{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81f5a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting split-folders\n",
      "  Using cached split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: split-folders\n",
      "Successfully installed split-folders-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c967b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.0.0+cu118\n",
      "Uninstalling torch-2.0.0+cu118:\n",
      "  Successfully uninstalled torch-2.0.0+cu118\n",
      "Found existing installation: torchvision 0.15.1+cu118\n",
      "Uninstalling torchvision-0.15.1+cu118:\n",
      "  Successfully uninstalled torchvision-0.15.1+cu118\n",
      "Found existing installation: torchaudio 2.0.1+cu118\n",
      "Uninstalling torchaudio-2.0.1+cu118:\n",
      "  Successfully uninstalled torchaudio-2.0.1+cu118\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a731779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1843.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m953.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-2.0.2%2Bcu117-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3.11/site-packages (from torch) (4.5.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Collecting triton==2.0.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting cmake (from triton==2.0.0->torch)\n",
      "  Using cached https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "Requirement already satisfied: lit in /usr/lib/python3.11/site-packages (from triton==2.0.0->torch) (15.0.7.dev0)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.11/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3.11/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.11/site-packages (from requests->torchvision) (1.26.15)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Installing collected packages: mpmath, cmake, sympy, networkx, filelock, triton, torch, torchvision, torchaudio\n",
      "Successfully installed cmake-3.25.0 filelock-3.9.0 mpmath-1.2.1 networkx-3.0 sympy-1.11.1 torch-2.0.1+cu117 torchaudio-2.0.2+cu117 torchvision-0.15.2+cu117 triton-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9e73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import splitfolders\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e9c778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9677fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder ='./Bottle Images'\n",
    "splitfolders.ratio(input_folder, output = \"splitted_bootles\", seed = 42, ratio = (.7, .2, .1), group_prefix = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f88786",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path = './splitted_bootles/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60af0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc4f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = torchvision.datasets.ImageFolder(root = training_dataset_path, transform = training_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e76c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset = training_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f4acec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        image_count_in_a_batch = images.size(0)\n",
    "        images = images.view(image_count_in_a_batch, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += image_count_in_a_batch\n",
    "        \n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efa079d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.4729, 0.4099, 0.3521]), tensor([0.1786, 0.1670, 0.1610]))\n"
     ]
    }
   ],
   "source": [
    "mean_and_std = get_mean_and_std(training_loader)\n",
    "print(mean_and_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa1e6b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4729, 0.4099, 0.3521])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_and_std[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6115b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1786, 0.1670, 0.1610])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_and_std[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8489bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_and_std = (torch.Tensor([0.4729, 0.4099, 0.3521]), torch.Tensor([0.1786, 0.1670, 0.1610]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b34c90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path = './splitted_bootles/train'\n",
    "validation_dataset_path = './splitted_bootles/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed25f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_and_std[0], mean_and_std[1])\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_and_std[0], mean_and_std[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c31ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = torchvision.datasets.ImageFolder(root = training_dataset_path, transform = training_transforms)\n",
    "validation_dataset = torchvision.datasets.ImageFolder(root = validation_dataset_path, transform = validation_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04487152",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset = training_dataset, batch_size = 32, shuffle = True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5693d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "868b11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, best_acc, training_loader, validation_loader, criterion, optimizer, n_epochs):\n",
    "    device = set_device()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch number %d\" % (epoch + 1))\n",
    "        model.train()\n",
    "        running_loss = 0.\n",
    "        running_correct = 0.\n",
    "        total = 0\n",
    "        \n",
    "        for data in training_loader:\n",
    "           \n",
    "            images, labels = data\n",
    "            \n",
    "            images = images.to(device)\n",
    "           \n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "           \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "          \n",
    "            loss.backward()\n",
    "           \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels == predicted).sum().item()\n",
    "            \n",
    "        \n",
    "        epoch_loss = running_loss/len(training_loader)\n",
    "        epoch_acc = 100.0 * running_correct / total\n",
    "        \n",
    "        print(\"    - Training dataset. Got %d out of %d images correctly (%.3f%%). Epoch loss: %.3f\"\n",
    "             % (running_correct, total, epoch_acc, epoch_loss))\n",
    "        \n",
    "        test_dataset_acc = evaluate_model_on_validation_set(model, validation_loader)\n",
    "        \n",
    "        if(test_dataset_acc > best_acc):\n",
    "            best_acc = test_dataset_acc\n",
    "            save_best_state(model, epoch, optimizer, best_acc)\n",
    "        \n",
    "    print(\"    Finished\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb2602db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_validation_set(model, validation_loader):\n",
    "    model.eval()\n",
    "    predicted_correctly_on_epoch = 0\n",
    "    total = 0\n",
    "    device = set_device()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total += labels.size(0)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted_correctly_on_epoch += (labels == predicted).sum().item()\n",
    "    epoch_acc = 100.0 * predicted_correctly_on_epoch / total\n",
    "    print(\"    - Testing dataset. Got %d out of %d images correctly (%.3f%%)\"\n",
    "         % (predicted_correctly_on_epoch, total, epoch_acc))\n",
    "    return epoch_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26d226c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_state(model, epoch, optimizer, best_acc):\n",
    "    state = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(),\n",
    "        'best_accuracy': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'date': datetime.now()\n",
    "    }\n",
    "    torch.save(state, 'best_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40ec41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "resnet18_model = models.resnet18(weights=None)\n",
    "num_ftrs = resnet18_model.fc.in_features\n",
    "resnet18_model.fc = nn.Linear(num_ftrs, 5) # 5 - number of classes\n",
    "device = set_device()\n",
    "resnet18_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet18_model.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36b943e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.22\n",
      "Epoch number 1\n",
      "    - Training dataset. Got 16877 out of 17500 images correctly (96.440%). Epoch loss: 0.115\n",
      "    - Testing dataset. Got 4833 out of 5000 images correctly (96.660%)\n",
      "    Finished\n",
      "5\n",
      "98.22\n",
      "2023-06-06 15:33:03.755987\n"
     ]
    }
   ],
   "source": [
    "acc = 0.\n",
    "n_epoch = 1\n",
    "if (torch.load('best_model.pth.tar')):\n",
    "    best = torch.load('best_model.pth.tar')\n",
    "    acc = best['best_accuracy']\n",
    "    optimizer.load_state_dict(best['optimizer'])\n",
    "    resnet18_model.load_state_dict(best['model'])\n",
    "    \n",
    "print(acc)\n",
    "\n",
    "train_nn(resnet18_model, acc, training_loader, validation_loader, loss_fn, optimizer, n_epoch)\n",
    "best = torch.load('best_model.pth.tar')\n",
    "print(best['epoch'])\n",
    "print(best['best_accuracy'])\n",
    "print(best['date'])\n",
    "resnet18_model.load_state_dict(best['model'])\n",
    "torch.save(resnet18_model, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f65b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
